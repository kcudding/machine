---
title: 'Using accelerometer data to identify correct barbell lifts'

---
###Executive summary
I used a random forest routine to create a model that could identify correct barbell lifts using data from accelerometers on the belt, forearm, arm, and dumbell.

```{r, echo=FALSE, warnings=FALSE, message=FALSE}
rm(list = ls())
library(caret)
library(ggplot2)
library(knitr)
library(randomForest)
options(digits=2)
set.seed(125)

```
## Data preprocessing

A preliminary examinination of the data indicated that many of the 160 columns had missing entries for all but 406 of the 19622 rows. The data were initially preprocessed to remove those columns that were NA for the majority of the rows of the dataset.

In addition, an examination of the remaining data columns suggested "classe", "new_window" and "user_name" should be treated as factors, while the remaining columns could be treated as numeric or integer. Summary data also indicated that simple normalizing transformations such as log10 would not be appropriate for most of the data since negative values were common.

```{r, fig.height=3.5, fig.width=7, tidy=TRUE, echo=TRUE, warnings=FALSE, message=FALSE}

training<-read.csv("~/kim/hopkinsdata/machine/pml-training.csv", stringsAsFactors = FALSE,
                  strip.white = TRUE, na.strings = c("NA","") )
store<-numeric()
for (i in 1:ncol(training)) 
  {
  if (sum(is.na(training[i]))>=(nrow(training)-406)) store<-c(store, i)
}

ntrain<-training[,-store]
ntrain<-ntrain[,-1]
ntrain$classe<-as.factor(ntrain$classe)
ntrain$new_window<-as.factor(ntrain$new_window)
ntrain$user_name<-as.factor(ntrain$user_name)
```
## Feature selection

Since the ojective is to correctly classify barbell lifts as belonging to various categories of correct or incorrect technqiue, I used a random forest routine to build a model. I initially used all potential features except "new_window". This factor variable produced errors in the randomForest routine.  I then examined the importance of each potential predictive feature

```{r, fig.height=6, fig.width=5, tidy=TRUE, cache=TRUE}
modFit<-randomForest(ntrain[-c(4,59)],ntrain$classe)
modFit
varImpPlot(modFit)
```

To try and prevent overfitting, I then selected a subset of features that were ranked as particularly important, and produced another random forest model using this subset of features. I reduced the number of trees to be used with this much smaller subset of predictors. I examined the prediction error given by the confusion matrix, and again looked at the relative importance of the predictors. 

```{r, fig.height=5, fig.width=5, tidy=TRUE, echo=FALSE, cache=TRUE}
topten<-modFit$importance[order(modFit$importance, decreasing=TRUE)[1:7]]
imp<-subset(modFit$importance, modFit$importance %in% topten)
rtrain<-rownames(imp)
modFit2<-randomForest(ntrain[rtrain],ntrain$classe, ntree=30)
modFit2
varImpPlot(modFit2)
```

## Final Model
Since prediction error was still quite low, I once again selected a subset of the important features, and generated the final model, once again using a reduced number of trees. 

```{r, fig.height=5, fig.width=5, tidy=TRUE, echo=FALSE, cache=TRUE}

topfive<-modFit2$importance[order(modFit2$importance, decreasing=TRUE)[1:4]]
imp2<-subset(modFit2$importance, modFit2$importance %in% topfive)
rtrain2<-rownames(imp2)
modFit3<-randomForest(ntrain[rtrain2],ntrain$classe, ntree=10)
modFit3
varImpPlot(modFit3)

```

##Cross validation and out of sample error

According the the authors Leo Breiman and Adele Cutler (https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr), in random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. Instead the out of sample error is generated by the routine. For each tree constructed ~30% of the cases are left out of the bootstrap sample and not used in the construction. Then each case not used in the construction of the tree is classified by the tree, and so a test classification set is generated and the prediction error for this test set is generated (out-of-bag error), which should be an unbiased estimator of out of sample error (0.06%).

